{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2f93f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from paz.backend.image import load_image, show_image\n",
    "from paz.applications import HaarCascadeFrontalFace, MiniXceptionFER\n",
    "import paz.processors as pr\n",
    "from paz.backend.image.image import crop_image\n",
    "from paz.processors.detection import CropBoxes2D\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from paz.abstract import SequentialProcessor\n",
    "from retinaface import RetinaFace\n",
    "from paz.abstract.messages import Box2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00102b86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Frame 11------\n",
      "Boxes:  [Box2D(497, 307, 717, 576, 0, None)] <class 'list'>\n",
      "[11, 'sad']\n",
      "--------Frame 12------\n",
      "Boxes:  [Box2D(497, 311, 718, 578, 0, None)] <class 'list'>\n",
      "[12, 'sad']\n",
      "--------Frame 13------\n",
      "Boxes:  [Box2D(498, 309, 717, 580, 0, None)] <class 'list'>\n",
      "[13, 'sad']\n",
      "--------Frame 14------\n",
      "Boxes:  [Box2D(497, 313, 718, 580, 0, None)] <class 'list'>\n",
      "[14, 'sad']\n",
      "--------Frame 15------\n",
      "Boxes:  [Box2D(498, 312, 717, 582, 0, None)] <class 'list'>\n",
      "[15, 'sad']\n",
      "--------Frame 16------\n",
      "Boxes:  [Box2D(497, 313, 716, 584, 0, None)] <class 'list'>\n",
      "[16, 'sad']\n",
      "--------Frame 17------\n",
      "Boxes:  [Box2D(497, 314, 716, 585, 0, None)] <class 'list'>\n",
      "[17, 'sad']\n",
      "--------Frame 18------\n",
      "Boxes:  [Box2D(497, 314, 716, 586, 0, None)] <class 'list'>\n",
      "[18, 'sad']\n",
      "--------Frame 19------\n",
      "Boxes:  [Box2D(498, 314, 715, 590, 0, None)] <class 'list'>\n",
      "[19, 'neutral']\n",
      "----------------Done getting emotion------------------\n",
      "Front face emotion labeling:  [['0' '0']\n",
      " ['11' 'sad']\n",
      " ['12' 'sad']\n",
      " ['13' 'sad']\n",
      " ['14' 'sad']\n",
      " ['15' 'sad']\n",
      " ['16' 'sad']\n",
      " ['17' 'sad']\n",
      " ['18' 'sad']\n",
      " ['19' 'neutral']] (10, 2)\n"
     ]
    }
   ],
   "source": [
    "class process_front_images():\n",
    "    def __init__(self):\n",
    "        self.folder_path = Path('DMD\\s3_rgb_face').__str__()\n",
    "        self.result = np.array([[0, 0]]) # 2D array with element [frame_number, emotion]\n",
    "        self.frame_offset = 10\n",
    "    def get_emotion(self):\n",
    "        for frame_number in range(1+self.frame_offset, 10+self.frame_offset): # starts from 0 + frame_offset\n",
    "            path = self.folder_path + '\\%(number)06d.png' % {\"number\": frame_number}\n",
    "            image = load_image(path)\n",
    "    \n",
    "            print('--------Frame {fnumber}------'.format(fnumber = frame_number))\n",
    "            crop = CropBoxes2D()\n",
    "            face = RetinaFace.detect_faces(path)\n",
    "            coordinates = face['face_1']['facial_area']\n",
    "            boxes2D = [Box2D(coordinates, score=0)]\n",
    "            print('Boxes: ', boxes2D, type(boxes2D))\n",
    "            if (len(boxes2D) == 0):\n",
    "                self.result = np.append(self.result, [[frame_number, 'nofacedetected']], axis=0)\n",
    "                continue\n",
    "                \n",
    "            elif (len(boxes2D) >= 2):\n",
    "                areas = np.array([])\n",
    "                for box in boxes2D:\n",
    "                    area = (box.coordinates[2]-box.coordinates[0]) * (box.coordinates[3]-box.coordinates[1])\n",
    "                    areas = np.append(areas, area)\n",
    "                indices_excluding_max = np.where(areas != np.amax(areas))[0]\n",
    "                print('Indices to remove', indices_excluding_max)\n",
    "                for i in indices_excluding_max:\n",
    "                    boxes2D = np.delete(boxes2D, i)\n",
    "                print('After removing smaller boxes: ', boxes2D)            \n",
    "            \n",
    "            cropped_images = crop(image, boxes2D)\n",
    "            #show_image(cropped_images[0])\n",
    "            classify = MiniXceptionFER()\n",
    "            emotion = classify(cropped_images[0])['class_name']\n",
    "            print([frame_number, emotion])\n",
    "            self.result = np.append(self.result, [[frame_number, emotion]], axis=0)\n",
    "        print('----------------Done getting emotion------------------')\n",
    "        return self.result\n",
    "    \n",
    "front_image_processor = process_front_images()\n",
    "emotion_result = front_image_processor.get_emotion()\n",
    "print('Front face emotion labeling: ', emotion_result, emotion_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cc44b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Frame 1------\n",
      "Boxes:  [Box2D(393, 25, 541, 238, 0, None)] <class 'list'>\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 11 is out of bounds for axis 0 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(name)\n\u001b[0;32m     41\u001b[0m side_processor \u001b[38;5;241m=\u001b[39m process_side_images()\n\u001b[1;32m---> 42\u001b[0m \u001b[43mside_processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_face_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 32\u001b[0m, in \u001b[0;36mprocess_side_images.get_face_image\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     29\u001b[0m         boxes2D \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdelete(boxes2D, i)\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAfter removing smaller boxes: \u001b[39m\u001b[38;5;124m'\u001b[39m, boxes2D)            \n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[43memotion_result\u001b[49m\u001b[43m[\u001b[49m\u001b[43mframe_number\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_offset\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnofacedetected\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult, [[frame_number, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnofacedetected\u001b[39m\u001b[38;5;124m'\u001b[39m]], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:   \u001b[38;5;66;03m# both dataset have face detected for this frame number              \u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 11 is out of bounds for axis 0 with size 10"
     ]
    }
   ],
   "source": [
    "class process_side_images():\n",
    "    def __init__(self):\n",
    "        self.folder_path = Path('DMD\\s3_rgb_body').__str__()\n",
    "        self.result = np.array([['0', '0']]) # path format?\n",
    "        self.frame_offset = 10\n",
    "    def get_face_image(self):\n",
    "        for frame_number in range(1, 10):\n",
    "            path = self.folder_path + '\\%(number)06d.png' % {\"number\": frame_number}\n",
    "            image = load_image(path)\n",
    "    \n",
    "            print('--------Frame {fnumber}------'.format(fnumber = frame_number))\n",
    "            crop = CropBoxes2D()\n",
    "            face = RetinaFace.detect_faces(path)\n",
    "            coordinates = face['face_1']['facial_area']\n",
    "            boxes2D = [Box2D(coordinates, score=0)]\n",
    "            print('Boxes: ', boxes2D, type(boxes2D))\n",
    "            if (len(boxes2D) == 0):\n",
    "                self.result = np.append(self.result, [[frame_number, 'nofacedetected']], axis=0)\n",
    "                continue\n",
    "                \n",
    "            elif (len(boxes2D) >= 2):\n",
    "                areas = np.array([])\n",
    "                for box in boxes2D:\n",
    "                    area = (box.coordinates[2]-box.coordinates[0]) * (box.coordinates[3]-box.coordinates[1])\n",
    "                    areas = np.append(areas, area)\n",
    "                indices_excluding_max = np.where(areas != np.amax(areas))[0]\n",
    "                print('Indices to remove', indices_excluding_max)\n",
    "                for i in indices_excluding_max:\n",
    "                    boxes2D = np.delete(boxes2D, i)\n",
    "                print('After removing smaller boxes: ', boxes2D)            \n",
    "            \n",
    "            if (emotion_result[frame_number + self.frame_offset][1] == 'nofacedetected'):\n",
    "                    self.result = np.append(self.result, [[frame_number, 'nofacedetected']], axis=0)\n",
    "            \n",
    "            else:   # both dataset have face detected for this frame number              \n",
    "                cropped_images = crop(image, boxes2D)\n",
    "                #show_image(cropped_images[0])\n",
    "                name = 'img.%(number)06d.png'% {'number': frame_number}\n",
    "                cropped_images.save(name)\n",
    "                print(name)\n",
    "side_processor = process_side_images()\n",
    "side_processor.get_face_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf4ab2e",
   "metadata": {},
   "source": [
    "### Code for detecting faces using PAZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a0a33b",
   "metadata": {},
   "source": [
    "Poor detection performance\n",
    "    4 out of 5 images were detected wrongly - boxes not appears around the face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22eb12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = self.folder_path + '\\%(number)06d.png' % {\"number\": frame_number}\n",
    "            image = load_image(path)\n",
    "    \n",
    "            print('--------Frame {fnumber}------'.format(fnumber = frame_number))\n",
    "            detect = HaarCascadeFrontalFace(draw=False)\n",
    "            crop = CropBoxes2D()\n",
    "            inference = detect(image)\n",
    "            boxes2D = inference['boxes2D']\n",
    "            print('Boxes: ', boxes2D, type(boxes2D))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
